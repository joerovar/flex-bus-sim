{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from objects import EnvironmentManager\n",
    "from helpers import *\n",
    "\n",
    "env = EnvironmentManager()\n",
    "env.start_vehicles()\n",
    "env.route.load_all_pax()\n",
    "\n",
    "observation, reward, terminated, truncated, info = env.step(action=None)\n",
    "while not terminated:\n",
    "    action = get_action('RA')\n",
    "    observation, reward, terminated, truncated, info = env.step(action=action)\n",
    "\n",
    "history = env.get_history()\n",
    "# for key in history:\n",
    "#     history[key]['scenario'] = scenario\n",
    "#     history[key]['episode'] = i\n",
    "#     results[key].append(history[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl_env import FlexSimEnv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_env  = FlexSimEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'control_stop_idx': array([1], dtype=int32),\n",
       "  'n_requests': array([1], dtype=int32),\n",
       "  'headway': array([666.], dtype=float32),\n",
       "  'schedule_deviation': array([-18.], dtype=float32)},\n",
       " {'skipped_requests': 0,\n",
       "  'off_schedule_trips': 0,\n",
       "  'time': 894.0,\n",
       "  'veh_idx': 1,\n",
       "  'direction': 'out'})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'control_stop_idx': array([0], dtype=int32),\n",
       "  'n_requests': array([3], dtype=int32),\n",
       "  'headway': array([593.], dtype=float32),\n",
       "  'schedule_deviation': array([-21.], dtype=float32)},\n",
       " -2.0,\n",
       " 0,\n",
       " 0,\n",
       " {'skipped_requests': 2,\n",
       "  'off_schedule_trips': 0,\n",
       "  'time': 5537.0,\n",
       "  'veh_idx': 1,\n",
       "  'direction': 'out'})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_env.step(action=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'skipped_requests': -1.0, 'off_schedule_trips': -1.0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from params import REWARD_WEIGHTS\n",
    "REWARD_WEIGHTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with shared policy...\n",
      "Episode 0/500, Avg Score: -0.48, Epsilon: 1.0000\n",
      "Episode 20/500, Avg Score: -0.69, Epsilon: 0.8521\n",
      "Episode 40/500, Avg Score: -0.66, Epsilon: 0.7261\n",
      "Episode 60/500, Avg Score: -0.63, Epsilon: 0.6187\n",
      "Episode 80/500, Avg Score: -0.70, Epsilon: 0.5280\n",
      "Episode 100/500, Avg Score: -0.57, Epsilon: 0.4501\n",
      "Episode 120/500, Avg Score: -0.51, Epsilon: 0.3836\n",
      "Episode 140/500, Avg Score: -0.48, Epsilon: 0.3268\n",
      "Episode 160/500, Avg Score: -0.60, Epsilon: 0.2785\n",
      "Episode 180/500, Avg Score: -0.47, Epsilon: 0.2374\n",
      "Episode 200/500, Avg Score: -0.56, Epsilon: 0.2023\n",
      "Episode 220/500, Avg Score: -0.42, Epsilon: 0.1724\n",
      "Episode 240/500, Avg Score: -0.44, Epsilon: 0.1469\n",
      "Episode 260/500, Avg Score: -0.38, Epsilon: 0.1252\n",
      "Episode 280/500, Avg Score: -0.41, Epsilon: 0.1067\n",
      "Episode 300/500, Avg Score: -0.43, Epsilon: 0.0909\n",
      "Episode 320/500, Avg Score: -0.41, Epsilon: 0.0774\n",
      "Episode 340/500, Avg Score: -0.47, Epsilon: 0.0660\n",
      "Episode 360/500, Avg Score: -0.34, Epsilon: 0.0562\n",
      "Episode 380/500, Avg Score: -0.51, Epsilon: 0.0500\n",
      "Episode 400/500, Avg Score: -0.45, Epsilon: 0.0500\n",
      "Episode 420/500, Avg Score: -0.43, Epsilon: 0.0500\n",
      "Episode 440/500, Avg Score: -0.43, Epsilon: 0.0500\n",
      "Episode 460/500, Avg Score: -0.37, Epsilon: 0.0500\n",
      "Episode 480/500, Avg Score: -0.39, Epsilon: 0.0500\n",
      "\n",
      "Training complete! Final average reward: -0.40\n",
      "Model saved as 'shared_dqn_agent.pth'\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "from training_claude import train_vehicles\n",
    "\n",
    "agent, scores = train_vehicles(reward_weights={'off_schedule_trips': -4.0, 'skipped_requests': -1.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For state {'control_stop_idx': array([0]), 'n_requests': array([2]), 'headway': array([700.5]), 'schedule_deviation': array([10])}, the model recommends action: 0\n"
     ]
    }
   ],
   "source": [
    "from training_claude import load_agent\n",
    "import numpy as np\n",
    "\n",
    "# Example of loading and using the model\n",
    "loaded_agent = load_agent('shared_dqn_agent.pth')\n",
    "\n",
    "# Example of getting an action from a state\n",
    "# The state should be a dictionary with the expected keys\n",
    "state = {\n",
    "    \"control_stop_idx\": np.array([0]),      # Make sure this is an array\n",
    "    \"n_requests\": np.array([2]),            # Make sure this is an array\n",
    "    \"headway\": np.array([700.5]),            # Make sure this is an array\n",
    "    \"schedule_deviation\": np.array([10])  # Make sure this is an array\n",
    "}\n",
    "vehicle_idx = 0  # Example vehicle index\n",
    "\n",
    "# Get action from loaded model\n",
    "action = loaded_agent.act(state, eval_mode=True)\n",
    "print(f\"For state {state}, the model recommends action: {action}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl_env import FlexSimEnv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(env, agent, num_episodes=10):\n",
    "    \"\"\"Evaluate the agent's performance in the environment.\"\"\"\n",
    "    results = {'pax': [], 'vehicles': [], 'state': [], 'idle': []}\n",
    "\n",
    "    np.random.seed(0)\n",
    "    for episode in range(num_episodes):\n",
    "        # Initialize state tracking for each vehicle\n",
    "        vehicle_observations = {}\n",
    "        vehicle_actions = {}  # Track previous actions\n",
    "        \n",
    "        # Start the episode\n",
    "        next_observation, info = env.reset()\n",
    "        vehicle_idx = info['veh_idx']\n",
    "\n",
    "        # update observation\n",
    "        observation = next_observation\n",
    "        vehicle_observations[vehicle_idx] = observation\n",
    "        \n",
    "        # select action\n",
    "        action = agent.act(observation, vehicle_idx)\n",
    "        vehicle_actions[vehicle_idx] = action\n",
    "\n",
    "        # take action in environment\n",
    "        next_observation, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        while not done:\n",
    "            # Get current vehicle index\n",
    "            vehicle_idx = info['veh_idx']\n",
    "            \n",
    "            # update observation\n",
    "            observation = next_observation\n",
    "            vehicle_observations[vehicle_idx] = observation\n",
    "\n",
    "            # Select action using shared policy\n",
    "            action = agent.act(observation, vehicle_idx)\n",
    "            vehicle_actions[vehicle_idx] = action\n",
    "            \n",
    "            # Take action in environment\n",
    "            next_observation, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "        # recordings\n",
    "        history = env.env.get_history()\n",
    "        for key in history:\n",
    "            history[key]['scenario'] = 'RL'\n",
    "            history[key]['episode'] = episode\n",
    "            results[key].append(history[key])\n",
    "    for df_key in results:\n",
    "        results[df_key] = pd.concat(results[df_key])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from params import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = FlexSimEnv()\n",
    "results = evaluate_agent(env, loaded_agent, num_episodes=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_field_from_list_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = results['state'].copy()\n",
    "state = state[state['time'].between(RESULTS_START_TIME_MINUTES*60, RESULTS_END_TIME_MINUTES*60)]\n",
    "state['unweighted_rewards'] = state['unweighted_rewards'].astype(str)\n",
    "create_field_from_list_column(state, 0, 'skipped_requests', field_name='unweighted_rewards')\n",
    "create_field_from_list_column(state, 1, 'off_schedule_trips', field_name='unweighted_rewards')\n",
    "state['weighted_reward'] = state['skipped_requests'] * -1.0 + state['off_schedule_trips'] * -4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scenario\n",
       "RL    718\n",
       "Name: action, dtype: int64"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.groupby(['scenario'])['action'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1327.0"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state['skipped_requests'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111.0"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state['off_schedule_trips'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2360, 11)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1771.0"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state['weighted_reward'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "recorded_actions = []\n",
    "recorded_observations = []\n",
    "rl_env = FlexSimEnv()\n",
    "\n",
    "observation, info = rl_env.reset()\n",
    "vehicle_idx = info['veh_idx']\n",
    "action = loaded_agent.act(observation, vehicle_idx, eval_mode=True)\n",
    "\n",
    "observation, reward, terminated, truncated, info = rl_env.step(action=action)\n",
    "while not terminated:\n",
    "    # Use the loaded agent to predict the action based on current observation\n",
    "    action = loaded_agent.act(observation, vehicle_idx, eval_mode=True)\n",
    "    recorded_actions.append(action)\n",
    "    recorded_observations.append(observation)\n",
    "    observation, reward, terminated, truncated, info = rl_env.step(action=action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'skipped_requests': 0, 'off_schedule_trips': 0}\n",
      "{'skipped_requests': 0, 'off_schedule_trips': 0}\n"
     ]
    }
   ],
   "source": [
    "for item in rl_env.env.route.inter_event:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_idxs = [i for i in range(len(recorded_actions)) if recorded_actions[i] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 40,\n",
       " 41,\n",
       " 44,\n",
       " 45,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 52,\n",
       " 55,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_env = FlexSimEnv()\n",
    "\n",
    "observation, info = rl_env.reset()\n",
    "action = model.predict(observation, deterministic=True)\n",
    "action = int(action[0])\n",
    "observation, reward, terminated, truncated, info = rl_env.step(action=action)\n",
    "while not terminated:\n",
    "    # Use the loaded agent to predict the action based on current observation\n",
    "    action = model.predict(observation, deterministic=True)\n",
    "    action = int(action[0])\n",
    "    observation, reward, terminated, truncated, info = rl_env.step(action=action)\n",
    "\n",
    "history = rl_env.env.get_history()\n",
    "for key in history:\n",
    "    history[key]['scenario'] = 'RL_' + model_name\n",
    "    history[key]['episode'] = i\n",
    "    results[key].append(history[key])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-bus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
