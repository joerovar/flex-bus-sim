{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from objects import EnvironmentManager\n",
    "from helpers import *\n",
    "\n",
    "env = EnvironmentManager()\n",
    "env.start_vehicles()\n",
    "env.route.load_all_pax()\n",
    "\n",
    "observation, reward, terminated, truncated, info = env.step(action=None)\n",
    "while not terminated:\n",
    "    action = get_action('RA')\n",
    "    observation, reward, terminated, truncated, info = env.step(action=action)\n",
    "\n",
    "history = env.get_history()\n",
    "# for key in history:\n",
    "#     history[key]['scenario'] = scenario\n",
    "#     history[key]['episode'] = i\n",
    "#     results[key].append(history[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl_env import FlexSimEnv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_env  = FlexSimEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'control_stop_idx': array([0], dtype=int32),\n",
       "  'n_requests': array([1], dtype=int32),\n",
       "  'headway': array([563.], dtype=float32),\n",
       "  'schedule_deviation': array([-56.], dtype=float32)},\n",
       " {'lost_requests': 0,\n",
       "  'off_schedule_trips': 0,\n",
       "  'total_trips': 0,\n",
       "  'time': 706.0,\n",
       "  'veh_idx': 1,\n",
       "  'direction': 'out'})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'control_stop_idx': array([3], dtype=int32),\n",
       "  'n_requests': array([1], dtype=int32),\n",
       "  'headway': array([506.], dtype=float32),\n",
       "  'schedule_deviation': array([-33.], dtype=float32)},\n",
       " 0.0,\n",
       " 0,\n",
       " 0,\n",
       " {'lost_requests': 0,\n",
       "  'off_schedule_trips': 0,\n",
       "  'total_trips': 0,\n",
       "  'time': 3877.0,\n",
       "  'veh_idx': 1,\n",
       "  'direction': 'in'})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_env.step(action=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m      3\u001b[0m     rl_env \u001b[38;5;241m=\u001b[39m FlexSimEnv()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "for i in range(2):\n",
    "    rl_env = FlexSimEnv()\n",
    "    observation, info = rl_env.reset()\n",
    "    # action = int(action[0])\n",
    "    observation, reward, terminated, truncated, info = rl_env.step(action=0)\n",
    "    while not terminated:\n",
    "        # Use the loaded agent to predict the action based on current observation\n",
    "        observation, reward, terminated, truncated, info = rl_env.step(action=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-02 10:56:30,402\tINFO worker.py:1684 -- Calling ray.init() again after it has already been called.\n",
      "/Users/joseph/miniconda3/envs/rl-bus/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py:512: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/Users/joseph/miniconda3/envs/rl-bus/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/Users/joseph/miniconda3/envs/rl-bus/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/Users/joseph/miniconda3/envs/rl-bus/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "\u001b[36m(MultiAgentEnvRunner pid=4320)\u001b[0m 2025-04-02 10:56:32,112\tWARNING env.py:37 -- Your MultiAgentEnv <SharedExperienceEnv<rllib-multi-agent-env-v0>> does not have some or all of the needed base-class attributes! Make sure you call `super().__init__()` from within your MutiAgentEnv's constructor. This will raise an error in the future.\n",
      "/Users/joseph/miniconda3/envs/rl-bus/lib/python3.11/site-packages/gymnasium/envs/registration.py:642: UserWarning: \u001b[33mWARN: Overriding environment rllib-multi-agent-env-v0 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n",
      "\u001b[36m(MultiAgentEnvRunner pid=4320)\u001b[0m 2025-04-02 10:56:32,250\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "2025-04-02 10:56:32,420\tWARNING util.py:61 -- Install gputil for GPU system monitoring.\n",
      "\u001b[36m(MultiAgentEnvRunner pid=4320)\u001b[0m 2025-04-02 10:56:32,423\tERROR actor_manager.py:187 -- Worker exception caught during `apply()`: not enough values to unpack (expected 2, got 1)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=4320)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(MultiAgentEnvRunner pid=4320)\u001b[0m   File \"/Users/joseph/miniconda3/envs/rl-bus/lib/python3.11/site-packages/ray/rllib/utils/actor_manager.py\", line 183, in apply\n",
      "\u001b[36m(MultiAgentEnvRunner pid=4320)\u001b[0m     return func(self, *args, **kwargs)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=4320)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=4320)\u001b[0m   File \"/Users/joseph/miniconda3/envs/rl-bus/lib/python3.11/site-packages/ray/rllib/execution/rollout_ops.py\", line 110, in <lambda>\n",
      "\u001b[36m(MultiAgentEnvRunner pid=4320)\u001b[0m     else (lambda w: (w.sample(**random_action_kwargs), w.get_metrics()))\n",
      "\u001b[36m(MultiAgentEnvRunner pid=4320)\u001b[0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=4320)\u001b[0m   File \"/Users/joseph/miniconda3/envs/rl-bus/lib/python3.11/site-packages/ray/util/tracing/tracing_helper.py\", line 463, in _resume_span\n",
      "\u001b[36m(MultiAgentEnvRunner pid=4320)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=4320)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=4320)\u001b[0m   File \"/Users/joseph/miniconda3/envs/rl-bus/lib/python3.11/site-packages/ray/rllib/env/multi_agent_env_runner.py\", line 209, in sample\n",
      "\u001b[36m(MultiAgentEnvRunner pid=4320)\u001b[0m     samples = self._sample(\n",
      "\u001b[36m(MultiAgentEnvRunner pid=4320)\u001b[0m               ^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=4320)\u001b[0m   File \"/Users/joseph/miniconda3/envs/rl-bus/lib/python3.11/site-packages/ray/util/tracing/tracing_helper.py\", line 463, in _resume_span\n",
      "\u001b[36m(MultiAgentEnvRunner pid=4320)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=4320)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=4320)\u001b[0m   File \"/Users/joseph/miniconda3/envs/rl-bus/lib/python3.11/site-packages/ray/rllib/env/multi_agent_env_runner.py\", line 264, in _sample\n",
      "\u001b[36m(MultiAgentEnvRunner pid=4320)\u001b[0m     self._reset_envs(episodes, shared_data, explore)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=4320)\u001b[0m   File \"/Users/joseph/miniconda3/envs/rl-bus/lib/python3.11/site-packages/ray/util/tracing/tracing_helper.py\", line 463, in _resume_span\n",
      "\u001b[36m(MultiAgentEnvRunner pid=4320)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=4320)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=4320)\u001b[0m   File \"/Users/joseph/miniconda3/envs/rl-bus/lib/python3.11/site-packages/ray/rllib/env/multi_agent_env_runner.py\", line 535, in _reset_envs\n",
      "\u001b[36m(MultiAgentEnvRunner pid=4320)\u001b[0m     observations, infos = self._try_env_reset()\n",
      "\u001b[36m(MultiAgentEnvRunner pid=4320)\u001b[0m                           ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=4320)\u001b[0m   File \"/Users/joseph/miniconda3/envs/rl-bus/lib/python3.11/site-packages/ray/util/tracing/tracing_helper.py\", line 463, in _resume_span\n",
      "\u001b[36m(MultiAgentEnvRunner pid=4320)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=4320)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=4320)\u001b[0m   File \"/Users/joseph/miniconda3/envs/rl-bus/lib/python3.11/site-packages/ray/rllib/env/env_runner.py\", line 187, in _try_env_reset\n",
      "\u001b[36m(MultiAgentEnvRunner pid=4320)\u001b[0m     raise e\n",
      "\u001b[36m(MultiAgentEnvRunner pid=4320)\u001b[0m   File \"/Users/joseph/miniconda3/envs/rl-bus/lib/python3.11/site-packages/ray/rllib/env/env_runner.py\", line 168, in _try_env_reset\n",
      "\u001b[36m(MultiAgentEnvRunner pid=4320)\u001b[0m     obs, infos = self.env.reset(\n",
      "\u001b[36m(MultiAgentEnvRunner pid=4320)\u001b[0m                  ^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=4320)\u001b[0m   File \"/Users/joseph/miniconda3/envs/rl-bus/lib/python3.11/site-packages/ray/rllib/env/vector/sync_vector_multi_agent_env.py\", line 80, in reset\n",
      "\u001b[36m(MultiAgentEnvRunner pid=4320)\u001b[0m     self._observations[i], self._infos[i] = env.reset(\n",
      "\u001b[36m(MultiAgentEnvRunner pid=4320)\u001b[0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=4320)\u001b[0m ValueError: not enough values to unpack (expected 2, got 1)\n",
      "2025-04-02 10:57:32,616\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffffec54a8b723e1fca4162c156601000000 Worker ID: ddf794f50831a6183d15d69613fdc1eb8aa96de06c9e7a587c9a51ec Node ID: 80baf190d189117b0a25003c668675e497da4d5ef53d4bc150a04d9d Worker IP address: 127.0.0.1 Worker port: 50908 Worker PID: 4320 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-02 10:57:34,143\tERROR actor_manager.py:873 -- Ray error (The actor ec54a8b723e1fca4162c156601000000 is unavailable: The actor is temporarily unavailable: IOError: The actor is restarting.. The task may or maynot have been executed on the actor.), taking actor 1 out of service.\n",
      "2025-04-02 10:57:34,144\tERROR actor_manager.py:873 -- Ray error (The actor 194f37903e54703cc23875af01000000 is unavailable: The actor is temporarily unavailable: IOError: The actor is restarting.. The task may or maynot have been executed on the actor.), taking actor 2 out of service.\n",
      "2025-04-02 10:57:34,144\tERROR actor_manager.py:674 -- The actor ec54a8b723e1fca4162c156601000000 is unavailable: The actor is temporarily unavailable: IOError: The actor is restarting.. The task may or maynot have been executed on the actor.\n",
      "NoneType: None\n",
      "2025-04-02 10:57:34,144\tERROR actor_manager.py:674 -- The actor 194f37903e54703cc23875af01000000 is unavailable: The actor is temporarily unavailable: IOError: The actor is restarting.. The task may or maynot have been executed on the actor.\n",
      "NoneType: None\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'episode_reward_mean'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 70\u001b[0m\n\u001b[1;32m     68\u001b[0m trainer \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mbuild()\n\u001b[1;32m     69\u001b[0m results \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 70\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Mean reward:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mresults\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepisode_reward_mean\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'episode_reward_mean'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MultiAgentEnvRunner pid=4536)\u001b[0m 2025-04-02 10:57:34,451\tWARNING env.py:37 -- Your MultiAgentEnv <SharedExperienceEnv<rllib-multi-agent-env-v0>> does not have some or all of the needed base-class attributes! Make sure you call `super().__init__()` from within your MutiAgentEnv's constructor. This will raise an error in the future.\n",
      "\u001b[36m(MultiAgentEnvRunner pid=4536)\u001b[0m 2025-04-02 10:57:34,591\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.algorithms.ppo import PPO\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "from ray.rllib.env.env_context import EnvContext\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# --- Minimal multi-agent env with shared policy ---\n",
    "class SharedExperienceEnv(MultiAgentEnv):\n",
    "    def __init__(self, config: EnvContext):\n",
    "        self._num_agents = config.get(\"num_agents\", 3)\n",
    "        self.agents = [f\"agent_{i}\" for i in range(self._num_agents)]\n",
    "        self.observation_spaces = {\n",
    "            agent: spaces.Box(low=-1, high=1, shape=(4,), dtype=np.float32)\n",
    "            for agent in self.agents\n",
    "        }\n",
    "        self.action_spaces = {\n",
    "            agent: spaces.Discrete(2)\n",
    "            for agent in self.agents\n",
    "        }\n",
    "        self.state_memory = {agent: np.zeros(4) for agent in self.agents}\n",
    "        self.current_agent = self.agents[0]\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        for agent in self.agents:\n",
    "            self.state_memory[agent] = np.random.randn(4)\n",
    "        self.current_agent = random.choice(self.agents)\n",
    "        return {self.current_agent: self.state_memory[self.current_agent]}\n",
    "\n",
    "    def step(self, action_dict):\n",
    "        agent_id = list(action_dict.keys())[0]\n",
    "        new_obs = np.random.randn(4)\n",
    "        reward = np.random.rand()\n",
    "        done = np.random.rand() < 0.05\n",
    "        self.state_memory[agent_id] = new_obs\n",
    "        next_agent = random.choice(self.agents)\n",
    "        self.current_agent = next_agent\n",
    "        return {agent_id: (new_obs, reward, done, {})}, {}, {}\n",
    "\n",
    "# --- Init Ray ---\n",
    "ray.init(ignore_reinit_error=True)\n",
    "\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n",
    "dummy_env = SharedExperienceEnv(EnvContext({\"num_agents\": 3}, worker_index=0))\n",
    "example_agent = dummy_env.agents[0]\n",
    "\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    .environment(env=SharedExperienceEnv, env_config={\"num_agents\": 3})\n",
    "    .multi_agent(\n",
    "        policies={\n",
    "            \"shared_policy\": (\n",
    "                None,\n",
    "                dummy_env.observation_spaces[example_agent],\n",
    "                dummy_env.action_spaces[example_agent],\n",
    "                {},\n",
    "            )\n",
    "        },\n",
    "        policy_mapping_fn=lambda agent_id: \"shared_policy\",\n",
    "    )\n",
    "    .framework(\"torch\")\n",
    ")\n",
    "\n",
    "trainer = config.build()\n",
    "results = trainer.train()\n",
    "print(\"✅ Mean reward:\", results[\"episode_reward_mean\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-bus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
