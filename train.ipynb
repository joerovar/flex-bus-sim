{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl_env import FlexSimEnv\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training_claude import train_vehicles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with shared policy...\n",
      "\n",
      "Training complete! Final average reward: -1.02\n",
      "\n",
      "Model saved as models/dqn_weight_2.pth\n",
      "Training with shared policy...\n",
      "\n",
      "Training complete! Final average reward: -1.18\n",
      "\n",
      "Model saved as models/dqn_weight_2_5.pth\n",
      "Training with shared policy...\n",
      "\n",
      "Training complete! Final average reward: -1.27\n",
      "\n",
      "Model saved as models/dqn_weight_3.pth\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 5e-3\n",
    "gamma = 0.98\n",
    "n_steps = 12_000\n",
    "\n",
    "# train with weight -6.0 and save \n",
    "agent, score = train_vehicles(\n",
    "    learning_rate=learning_rate,\n",
    "    gamma=gamma,\n",
    "    n_steps=n_steps,\n",
    "    reward_weight=-2.0,\n",
    "    save_path='models/dqn_weight_2.pth',\n",
    ")\n",
    "\n",
    "# train with weight -5.0 and save\n",
    "agent, score = train_vehicles(\n",
    "    learning_rate=learning_rate,\n",
    "    gamma=gamma,\n",
    "    n_steps=n_steps,\n",
    "    reward_weight=-2.5,\n",
    "    save_path='models/dqn_weight_2_5.pth',\n",
    ")\n",
    "\n",
    "# train with weight -5.0 and save\n",
    "agent, score = train_vehicles(\n",
    "    learning_rate=learning_rate,\n",
    "    gamma=gamma,\n",
    "    n_steps=n_steps,\n",
    "    reward_weight=-3.0,\n",
    "    save_path='models/dqn_weight_3.pth',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slope evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval import evaluate_slopes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating slope 0.0, reward_weight -2.0\n",
      "Evaluating slope 0.0, reward_weight -2.5\n",
      "Evaluating slope 0.0, reward_weight -3.0\n",
      "Evaluating slope 0.5, reward_weight -2.0\n",
      "Evaluating slope 0.5, reward_weight -2.5\n",
      "Evaluating slope 0.5, reward_weight -3.0\n",
      "Evaluating slope 1.0, reward_weight -2.0\n",
      "Evaluating slope 1.0, reward_weight -2.5\n",
      "Evaluating slope 1.0, reward_weight -3.0\n",
      "Evaluating slope 1.5, reward_weight -2.0\n",
      "Evaluating slope 1.5, reward_weight -2.5\n",
      "Evaluating slope 1.5, reward_weight -3.0\n",
      "Evaluating slope 2.0, reward_weight -2.0\n",
      "Evaluating slope 2.0, reward_weight -2.5\n",
      "Evaluating slope 2.0, reward_weight -3.0\n",
      "Evaluating slope 2.5, reward_weight -2.0\n",
      "Evaluating slope 2.5, reward_weight -2.5\n",
      "Evaluating slope 2.5, reward_weight -3.0\n",
      "Evaluating slope 3.0, reward_weight -2.0\n",
      "Evaluating slope 3.0, reward_weight -2.5\n",
      "Evaluating slope 3.0, reward_weight -3.0\n"
     ]
    }
   ],
   "source": [
    "reward_weights = [-2.0, -2.5, -3.0]\n",
    "slope_results = evaluate_slopes(reward_weights=reward_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope_df = pd.DataFrame(slope_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(slope_results).sort_values(by=['slope','mean_reward'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training_claude import grid_search_dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with shared policy...\n",
      "\n",
      "Training complete! Final average reward: -0.97\n",
      "\n",
      "Evaluation summary:\n",
      "Params: lr=0.005, n_steps=12000, gamma=0.98, reward_weight=-2.0\n",
      "Reward: -0.857 +/- 1.328\n",
      "------------------------\n",
      "Training with shared policy...\n",
      "\n",
      "Training complete! Final average reward: -1.19\n",
      "\n",
      "Evaluation summary:\n",
      "Params: lr=0.005, n_steps=12000, gamma=0.98, reward_weight=-2.5\n",
      "Reward: -0.622 +/- 1.103\n",
      "------------------------\n",
      "Training with shared policy...\n",
      "\n",
      "Training complete! Final average reward: -1.26\n",
      "\n",
      "Evaluation summary:\n",
      "Params: lr=0.005, n_steps=12000, gamma=0.98, reward_weight=-3.0\n",
      "Reward: -0.71 +/- 1.432\n",
      "------------------------\n",
      "Training with shared policy...\n",
      "\n",
      "Training complete! Final average reward: -0.95\n",
      "\n",
      "Evaluation summary:\n",
      "Params: lr=0.005, n_steps=12000, gamma=0.99, reward_weight=-2.0\n",
      "Reward: -0.53 +/- 1.083\n",
      "------------------------\n",
      "Training with shared policy...\n",
      "\n",
      "Training complete! Final average reward: -1.20\n",
      "\n",
      "Evaluation summary:\n",
      "Params: lr=0.005, n_steps=12000, gamma=0.99, reward_weight=-2.5\n",
      "Reward: -0.975 +/- 1.604\n",
      "------------------------\n",
      "Training with shared policy...\n",
      "\n",
      "Training complete! Final average reward: -1.23\n",
      "\n",
      "Evaluation summary:\n",
      "Params: lr=0.005, n_steps=12000, gamma=0.99, reward_weight=-3.0\n",
      "Reward: -0.842 +/- 1.305\n",
      "------------------------\n",
      "Training with shared policy...\n",
      "\n",
      "Training complete! Final average reward: -1.06\n",
      "\n",
      "Evaluation summary:\n",
      "Params: lr=0.005, n_steps=18000, gamma=0.98, reward_weight=-2.0\n",
      "Reward: -0.499 +/- 0.966\n",
      "------------------------\n",
      "Training with shared policy...\n",
      "\n",
      "Training complete! Final average reward: -1.19\n",
      "\n",
      "Evaluation summary:\n",
      "Params: lr=0.005, n_steps=18000, gamma=0.98, reward_weight=-2.5\n",
      "Reward: -1.114 +/- 1.726\n",
      "------------------------\n",
      "Training with shared policy...\n",
      "\n",
      "Training complete! Final average reward: -1.29\n",
      "\n",
      "Evaluation summary:\n",
      "Params: lr=0.005, n_steps=18000, gamma=0.98, reward_weight=-3.0\n",
      "Reward: -1.604 +/- 1.545\n",
      "------------------------\n",
      "Training with shared policy...\n",
      "\n",
      "Training complete! Final average reward: -1.04\n",
      "\n",
      "Evaluation summary:\n",
      "Params: lr=0.005, n_steps=18000, gamma=0.99, reward_weight=-2.0\n",
      "Reward: -0.946 +/- 1.37\n",
      "------------------------\n",
      "Training with shared policy...\n",
      "\n",
      "Training complete! Final average reward: -1.12\n",
      "\n",
      "Evaluation summary:\n",
      "Params: lr=0.005, n_steps=18000, gamma=0.99, reward_weight=-2.5\n",
      "Reward: -0.531 +/- 1.035\n",
      "------------------------\n",
      "Training with shared policy...\n",
      "\n",
      "Training complete! Final average reward: -1.37\n",
      "\n",
      "Evaluation summary:\n",
      "Params: lr=0.005, n_steps=18000, gamma=0.99, reward_weight=-3.0\n",
      "Reward: -1.422 +/- 2.059\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "grid_search_results = grid_search_dqn(\n",
    "    lr_values=[5e-3], # learning rate\n",
    "    n_steps_values=[12_000,18_000], # timesteps\n",
    "    gamma_values=[0.98, 0.99], # discount factor\n",
    "    reward_weights=[-2.0, -2.5, -3.0], # reward weights\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_df = pd.DataFrame(grid_search_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slope</th>\n",
       "      <th>reward_weight</th>\n",
       "      <th>mean_reward</th>\n",
       "      <th>std_reward</th>\n",
       "      <th>deviation_opportunities</th>\n",
       "      <th>deviations</th>\n",
       "      <th>avg_picked_requests</th>\n",
       "      <th>early_trips</th>\n",
       "      <th>late_trips</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>-1.322</td>\n",
       "      <td>1.993</td>\n",
       "      <td>35.6</td>\n",
       "      <td>35.6</td>\n",
       "      <td>1.5</td>\n",
       "      <td>6.9</td>\n",
       "      <td>23.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.5</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>-1.080</td>\n",
       "      <td>1.867</td>\n",
       "      <td>36.3</td>\n",
       "      <td>34.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>6.9</td>\n",
       "      <td>17.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>-0.908</td>\n",
       "      <td>1.687</td>\n",
       "      <td>38.1</td>\n",
       "      <td>34.1</td>\n",
       "      <td>1.6</td>\n",
       "      <td>6.5</td>\n",
       "      <td>14.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.5</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>-0.824</td>\n",
       "      <td>1.605</td>\n",
       "      <td>38.8</td>\n",
       "      <td>33.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>6.6</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>-0.785</td>\n",
       "      <td>1.533</td>\n",
       "      <td>38.6</td>\n",
       "      <td>31.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>6.6</td>\n",
       "      <td>9.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>-0.739</td>\n",
       "      <td>1.507</td>\n",
       "      <td>39.2</td>\n",
       "      <td>31.8</td>\n",
       "      <td>1.6</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2.5</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>-0.731</td>\n",
       "      <td>1.505</td>\n",
       "      <td>38.7</td>\n",
       "      <td>31.6</td>\n",
       "      <td>1.6</td>\n",
       "      <td>6.3</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>-1.101</td>\n",
       "      <td>1.661</td>\n",
       "      <td>35.6</td>\n",
       "      <td>35.6</td>\n",
       "      <td>1.5</td>\n",
       "      <td>6.9</td>\n",
       "      <td>23.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.5</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>-0.905</td>\n",
       "      <td>1.568</td>\n",
       "      <td>36.3</td>\n",
       "      <td>34.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>6.9</td>\n",
       "      <td>17.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>-0.768</td>\n",
       "      <td>1.423</td>\n",
       "      <td>38.1</td>\n",
       "      <td>34.1</td>\n",
       "      <td>1.6</td>\n",
       "      <td>6.5</td>\n",
       "      <td>14.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.5</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>-0.705</td>\n",
       "      <td>1.364</td>\n",
       "      <td>38.8</td>\n",
       "      <td>33.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>6.6</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>-0.676</td>\n",
       "      <td>1.307</td>\n",
       "      <td>38.6</td>\n",
       "      <td>31.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>6.6</td>\n",
       "      <td>9.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3.0</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>-0.642</td>\n",
       "      <td>1.297</td>\n",
       "      <td>39.2</td>\n",
       "      <td>31.8</td>\n",
       "      <td>1.6</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2.5</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>-0.632</td>\n",
       "      <td>1.291</td>\n",
       "      <td>38.7</td>\n",
       "      <td>31.6</td>\n",
       "      <td>1.6</td>\n",
       "      <td>6.3</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-0.881</td>\n",
       "      <td>1.328</td>\n",
       "      <td>35.6</td>\n",
       "      <td>35.6</td>\n",
       "      <td>1.5</td>\n",
       "      <td>6.9</td>\n",
       "      <td>23.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.5</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-0.730</td>\n",
       "      <td>1.270</td>\n",
       "      <td>36.3</td>\n",
       "      <td>34.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>6.9</td>\n",
       "      <td>17.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-0.627</td>\n",
       "      <td>1.161</td>\n",
       "      <td>38.1</td>\n",
       "      <td>34.1</td>\n",
       "      <td>1.6</td>\n",
       "      <td>6.5</td>\n",
       "      <td>14.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.5</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-0.586</td>\n",
       "      <td>1.126</td>\n",
       "      <td>38.8</td>\n",
       "      <td>33.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>6.6</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-0.566</td>\n",
       "      <td>1.086</td>\n",
       "      <td>38.6</td>\n",
       "      <td>31.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>6.6</td>\n",
       "      <td>9.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-0.544</td>\n",
       "      <td>1.093</td>\n",
       "      <td>39.2</td>\n",
       "      <td>31.8</td>\n",
       "      <td>1.6</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2.5</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-0.533</td>\n",
       "      <td>1.083</td>\n",
       "      <td>38.7</td>\n",
       "      <td>31.6</td>\n",
       "      <td>1.6</td>\n",
       "      <td>6.3</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    slope  reward_weight  mean_reward  std_reward  deviation_opportunities  \\\n",
       "2     0.0           -3.0       -1.322       1.993                     35.6   \n",
       "5     0.5           -3.0       -1.080       1.867                     36.3   \n",
       "8     1.0           -3.0       -0.908       1.687                     38.1   \n",
       "11    1.5           -3.0       -0.824       1.605                     38.8   \n",
       "14    2.0           -3.0       -0.785       1.533                     38.6   \n",
       "20    3.0           -3.0       -0.739       1.507                     39.2   \n",
       "17    2.5           -3.0       -0.731       1.505                     38.7   \n",
       "1     0.0           -2.5       -1.101       1.661                     35.6   \n",
       "4     0.5           -2.5       -0.905       1.568                     36.3   \n",
       "7     1.0           -2.5       -0.768       1.423                     38.1   \n",
       "10    1.5           -2.5       -0.705       1.364                     38.8   \n",
       "13    2.0           -2.5       -0.676       1.307                     38.6   \n",
       "19    3.0           -2.5       -0.642       1.297                     39.2   \n",
       "16    2.5           -2.5       -0.632       1.291                     38.7   \n",
       "0     0.0           -2.0       -0.881       1.328                     35.6   \n",
       "3     0.5           -2.0       -0.730       1.270                     36.3   \n",
       "6     1.0           -2.0       -0.627       1.161                     38.1   \n",
       "9     1.5           -2.0       -0.586       1.126                     38.8   \n",
       "12    2.0           -2.0       -0.566       1.086                     38.6   \n",
       "18    3.0           -2.0       -0.544       1.093                     39.2   \n",
       "15    2.5           -2.0       -0.533       1.083                     38.7   \n",
       "\n",
       "    deviations  avg_picked_requests  early_trips  late_trips  \n",
       "2         35.6                  1.5          6.9        23.7  \n",
       "5         34.5                  1.5          6.9        17.9  \n",
       "8         34.1                  1.6          6.5        14.1  \n",
       "11        33.0                  1.6          6.6        11.0  \n",
       "14        31.9                  1.7          6.6         9.6  \n",
       "20        31.8                  1.6          7.0         8.1  \n",
       "17        31.6                  1.6          6.3         8.5  \n",
       "1         35.6                  1.5          6.9        23.7  \n",
       "4         34.5                  1.5          6.9        17.9  \n",
       "7         34.1                  1.6          6.5        14.1  \n",
       "10        33.0                  1.6          6.6        11.0  \n",
       "13        31.9                  1.7          6.6         9.6  \n",
       "19        31.8                  1.6          7.0         8.1  \n",
       "16        31.6                  1.6          6.3         8.5  \n",
       "0         35.6                  1.5          6.9        23.7  \n",
       "3         34.5                  1.5          6.9        17.9  \n",
       "6         34.1                  1.6          6.5        14.1  \n",
       "9         33.0                  1.6          6.6        11.0  \n",
       "12        31.9                  1.7          6.6         9.6  \n",
       "18        31.8                  1.6          7.0         8.1  \n",
       "15        31.6                  1.6          6.3         8.5  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slope_df.sort_values(by=['reward_weight','mean_reward'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weight</th>\n",
       "      <th>lr</th>\n",
       "      <th>n_steps</th>\n",
       "      <th>gamma</th>\n",
       "      <th>mean_reward</th>\n",
       "      <th>std_reward</th>\n",
       "      <th>deviation_opportunities</th>\n",
       "      <th>deviations</th>\n",
       "      <th>avg_picked_requests</th>\n",
       "      <th>early_trips</th>\n",
       "      <th>late_trips</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-3.0</td>\n",
       "      <td>0.005</td>\n",
       "      <td>18000</td>\n",
       "      <td>0.98</td>\n",
       "      <td>-1.604</td>\n",
       "      <td>1.545</td>\n",
       "      <td>44.3</td>\n",
       "      <td>8.8</td>\n",
       "      <td>1.6</td>\n",
       "      <td>13.2</td>\n",
       "      <td>1.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-3.0</td>\n",
       "      <td>0.005</td>\n",
       "      <td>18000</td>\n",
       "      <td>0.99</td>\n",
       "      <td>-1.422</td>\n",
       "      <td>2.059</td>\n",
       "      <td>34.4</td>\n",
       "      <td>34.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>6.7</td>\n",
       "      <td>25.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-3.0</td>\n",
       "      <td>0.005</td>\n",
       "      <td>12000</td>\n",
       "      <td>0.99</td>\n",
       "      <td>-0.842</td>\n",
       "      <td>1.305</td>\n",
       "      <td>40.9</td>\n",
       "      <td>21.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>7.9</td>\n",
       "      <td>2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-3.0</td>\n",
       "      <td>0.005</td>\n",
       "      <td>12000</td>\n",
       "      <td>0.98</td>\n",
       "      <td>-0.710</td>\n",
       "      <td>1.432</td>\n",
       "      <td>37.8</td>\n",
       "      <td>29.8</td>\n",
       "      <td>1.6</td>\n",
       "      <td>7.4</td>\n",
       "      <td>6.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-2.5</td>\n",
       "      <td>0.005</td>\n",
       "      <td>18000</td>\n",
       "      <td>0.98</td>\n",
       "      <td>-1.114</td>\n",
       "      <td>1.726</td>\n",
       "      <td>36.8</td>\n",
       "      <td>35.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>5.5</td>\n",
       "      <td>24.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-2.5</td>\n",
       "      <td>0.005</td>\n",
       "      <td>12000</td>\n",
       "      <td>0.99</td>\n",
       "      <td>-0.975</td>\n",
       "      <td>1.604</td>\n",
       "      <td>35.4</td>\n",
       "      <td>35.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>5.7</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.5</td>\n",
       "      <td>0.005</td>\n",
       "      <td>12000</td>\n",
       "      <td>0.98</td>\n",
       "      <td>-0.622</td>\n",
       "      <td>1.103</td>\n",
       "      <td>40.9</td>\n",
       "      <td>27.5</td>\n",
       "      <td>1.7</td>\n",
       "      <td>6.4</td>\n",
       "      <td>3.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-2.5</td>\n",
       "      <td>0.005</td>\n",
       "      <td>18000</td>\n",
       "      <td>0.99</td>\n",
       "      <td>-0.531</td>\n",
       "      <td>1.035</td>\n",
       "      <td>38.4</td>\n",
       "      <td>27.3</td>\n",
       "      <td>1.7</td>\n",
       "      <td>6.2</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.005</td>\n",
       "      <td>18000</td>\n",
       "      <td>0.99</td>\n",
       "      <td>-0.946</td>\n",
       "      <td>1.370</td>\n",
       "      <td>33.6</td>\n",
       "      <td>33.6</td>\n",
       "      <td>1.6</td>\n",
       "      <td>7.2</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.005</td>\n",
       "      <td>12000</td>\n",
       "      <td>0.98</td>\n",
       "      <td>-0.857</td>\n",
       "      <td>1.328</td>\n",
       "      <td>35.2</td>\n",
       "      <td>35.2</td>\n",
       "      <td>1.5</td>\n",
       "      <td>6.1</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.005</td>\n",
       "      <td>12000</td>\n",
       "      <td>0.99</td>\n",
       "      <td>-0.530</td>\n",
       "      <td>1.083</td>\n",
       "      <td>38.6</td>\n",
       "      <td>33.5</td>\n",
       "      <td>1.6</td>\n",
       "      <td>5.7</td>\n",
       "      <td>10.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.005</td>\n",
       "      <td>18000</td>\n",
       "      <td>0.98</td>\n",
       "      <td>-0.499</td>\n",
       "      <td>0.966</td>\n",
       "      <td>38.1</td>\n",
       "      <td>30.7</td>\n",
       "      <td>1.6</td>\n",
       "      <td>6.9</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    weight     lr  n_steps  gamma  mean_reward  std_reward  \\\n",
       "8     -3.0  0.005    18000   0.98       -1.604       1.545   \n",
       "11    -3.0  0.005    18000   0.99       -1.422       2.059   \n",
       "5     -3.0  0.005    12000   0.99       -0.842       1.305   \n",
       "2     -3.0  0.005    12000   0.98       -0.710       1.432   \n",
       "7     -2.5  0.005    18000   0.98       -1.114       1.726   \n",
       "4     -2.5  0.005    12000   0.99       -0.975       1.604   \n",
       "1     -2.5  0.005    12000   0.98       -0.622       1.103   \n",
       "10    -2.5  0.005    18000   0.99       -0.531       1.035   \n",
       "9     -2.0  0.005    18000   0.99       -0.946       1.370   \n",
       "0     -2.0  0.005    12000   0.98       -0.857       1.328   \n",
       "3     -2.0  0.005    12000   0.99       -0.530       1.083   \n",
       "6     -2.0  0.005    18000   0.98       -0.499       0.966   \n",
       "\n",
       "    deviation_opportunities  deviations  avg_picked_requests  early_trips  \\\n",
       "8                      44.3         8.8                  1.6         13.2   \n",
       "11                     34.4        34.4                  1.6          6.7   \n",
       "5                      40.9        21.1                  1.8          7.9   \n",
       "2                      37.8        29.8                  1.6          7.4   \n",
       "7                      36.8        35.4                  1.5          5.5   \n",
       "4                      35.4        35.4                  1.5          5.7   \n",
       "1                      40.9        27.5                  1.7          6.4   \n",
       "10                     38.4        27.3                  1.7          6.2   \n",
       "9                      33.6        33.6                  1.6          7.2   \n",
       "0                      35.2        35.2                  1.5          6.1   \n",
       "3                      38.6        33.5                  1.6          5.7   \n",
       "6                      38.1        30.7                  1.6          6.9   \n",
       "\n",
       "    late_trips  \n",
       "8          1.6  \n",
       "11        25.1  \n",
       "5          2.2  \n",
       "2          6.5  \n",
       "7         24.6  \n",
       "4         21.6  \n",
       "1          3.8  \n",
       "10         4.2  \n",
       "9         25.0  \n",
       "0         24.0  \n",
       "3         10.6  \n",
       "6          7.0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_df.sort_values(by=['weight','mean_reward'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test default training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with shared policy...\n",
      "Episode 0/500, Avg Score: -0.48, Epsilon: 1.0000\n",
      "Episode 20/500, Avg Score: -0.69, Epsilon: 0.8521\n",
      "Episode 40/500, Avg Score: -0.66, Epsilon: 0.7261\n",
      "Episode 60/500, Avg Score: -0.63, Epsilon: 0.6187\n",
      "Episode 80/500, Avg Score: -0.70, Epsilon: 0.5280\n",
      "Episode 100/500, Avg Score: -0.57, Epsilon: 0.4501\n",
      "Episode 120/500, Avg Score: -0.51, Epsilon: 0.3836\n",
      "Episode 140/500, Avg Score: -0.48, Epsilon: 0.3268\n",
      "Episode 160/500, Avg Score: -0.60, Epsilon: 0.2785\n",
      "Episode 180/500, Avg Score: -0.47, Epsilon: 0.2374\n",
      "Episode 200/500, Avg Score: -0.56, Epsilon: 0.2023\n",
      "Episode 220/500, Avg Score: -0.42, Epsilon: 0.1724\n",
      "Episode 240/500, Avg Score: -0.44, Epsilon: 0.1469\n",
      "Episode 260/500, Avg Score: -0.38, Epsilon: 0.1252\n",
      "Episode 280/500, Avg Score: -0.41, Epsilon: 0.1067\n",
      "Episode 300/500, Avg Score: -0.43, Epsilon: 0.0909\n",
      "Episode 320/500, Avg Score: -0.41, Epsilon: 0.0774\n",
      "Episode 340/500, Avg Score: -0.47, Epsilon: 0.0660\n",
      "Episode 360/500, Avg Score: -0.34, Epsilon: 0.0562\n",
      "Episode 380/500, Avg Score: -0.51, Epsilon: 0.0500\n",
      "Episode 400/500, Avg Score: -0.45, Epsilon: 0.0500\n",
      "Episode 420/500, Avg Score: -0.43, Epsilon: 0.0500\n",
      "Episode 440/500, Avg Score: -0.43, Epsilon: 0.0500\n",
      "Episode 460/500, Avg Score: -0.37, Epsilon: 0.0500\n",
      "Episode 480/500, Avg Score: -0.39, Epsilon: 0.0500\n",
      "\n",
      "Training complete! Final average reward: -0.40\n",
      "Model saved as 'shared_dqn_agent.pth'\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "from training_claude import train_vehicles\n",
    "\n",
    "agent, scores = train_vehicles(reward_weights={'off_schedule_trips': -4.0, 'skipped_requests': -1.0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: turn this into a function that by control stop, and number of requests creates scatterplots for headway and schedule deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For state {'control_stop_idx': array([0]), 'n_requests': array([2]), 'headway': array([700.5]), 'schedule_deviation': array([10])}, the model recommends action: 0\n"
     ]
    }
   ],
   "source": [
    "from training_claude import load_agent\n",
    "import numpy as np\n",
    "\n",
    "# Example of loading and using the model\n",
    "loaded_agent = load_agent('shared_dqn_agent.pth')\n",
    "\n",
    "# Example of getting an action from a state\n",
    "# The state should be a dictionary with the expected keys\n",
    "state = {\n",
    "    \"control_stop_idx\": np.array([0]),      # Make sure this is an array\n",
    "    \"n_requests\": np.array([2]),            # Make sure this is an array\n",
    "    \"headway\": np.array([700.5]),            # Make sure this is an array\n",
    "    \"schedule_deviation\": np.array([10])  # Make sure this is an array\n",
    "}\n",
    "vehicle_idx = 0  # Example vehicle index\n",
    "\n",
    "# Get action from loaded model\n",
    "action = loaded_agent.act(state, eval_mode=True)\n",
    "print(f\"For state {state}, the model recommends action: {action}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl_env import FlexSimEnv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(env, agent, num_episodes=10):\n",
    "    \"\"\"Evaluate the agent's performance in the environment.\"\"\"\n",
    "    results = {'pax': [], 'vehicles': [], 'state': [], 'idle': []}\n",
    "\n",
    "    np.random.seed(0)\n",
    "    for episode in range(num_episodes):\n",
    "        # Initialize state tracking for each vehicle\n",
    "        vehicle_observations = {}\n",
    "        vehicle_actions = {}  # Track previous actions\n",
    "        \n",
    "        # Start the episode\n",
    "        next_observation, info = env.reset()\n",
    "        vehicle_idx = info['veh_idx']\n",
    "\n",
    "        # update observation\n",
    "        observation = next_observation\n",
    "        vehicle_observations[vehicle_idx] = observation\n",
    "        \n",
    "        # select action\n",
    "        action = agent.act(observation, vehicle_idx)\n",
    "        vehicle_actions[vehicle_idx] = action\n",
    "\n",
    "        # take action in environment\n",
    "        next_observation, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        while not done:\n",
    "            # Get current vehicle index\n",
    "            vehicle_idx = info['veh_idx']\n",
    "            \n",
    "            # update observation\n",
    "            observation = next_observation\n",
    "            vehicle_observations[vehicle_idx] = observation\n",
    "\n",
    "            # Select action using shared policy\n",
    "            action = agent.act(observation, vehicle_idx)\n",
    "            vehicle_actions[vehicle_idx] = action\n",
    "            \n",
    "            # Take action in environment\n",
    "            next_observation, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "        # recordings\n",
    "        history = env.env.get_history()\n",
    "        for key in history:\n",
    "            history[key]['scenario'] = 'RL'\n",
    "            history[key]['episode'] = episode\n",
    "            results[key].append(history[key])\n",
    "    for df_key in results:\n",
    "        results[df_key] = pd.concat(results[df_key])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "from params import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = FlexSimEnv()\n",
    "results = evaluate_agent(env, loaded_agent, num_episodes=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_field_from_list_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = results['state'].copy()\n",
    "state = state[state['time'].between(RESULTS_START_TIME_MINUTES*60, RESULTS_END_TIME_MINUTES*60)]\n",
    "state['unweighted_rewards'] = state['unweighted_rewards'].astype(str)\n",
    "create_field_from_list_column(state, 0, 'skipped_requests', field_name='unweighted_rewards')\n",
    "create_field_from_list_column(state, 1, 'off_schedule_trips', field_name='unweighted_rewards')\n",
    "state['weighted_reward'] = state['skipped_requests'] * -1.0 + state['off_schedule_trips'] * -4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_raw = results['state'].copy()\n",
    "# state_raw.groupby(['episode']).size().head() # confirm it's 65 control steps per episode\n",
    "num_time_steps = 10_000\n",
    "time_steps_per_episode = 65\n",
    "num_episodes = int(num_time_steps/time_steps_per_episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## toy with environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_env  = FlexSimEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<rl_env.FlexSimEnv at 0x171bcb690>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_env.route."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'control_stop_idx': array([0], dtype=int32),\n",
       "  'n_requests': array([0], dtype=int32),\n",
       "  'headway': array([594.], dtype=float32),\n",
       "  'schedule_deviation': array([-55.], dtype=float32)},\n",
       " {'skipped_requests': 0,\n",
       "  'off_schedule_trips': 0,\n",
       "  'time': 705.0,\n",
       "  'veh_idx': 1,\n",
       "  'direction': 'out'})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rl_env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'control_stop_idx': array([1], dtype=int32),\n",
       "  'n_requests': array([1], dtype=int32),\n",
       "  'headway': array([495.], dtype=float32),\n",
       "  'schedule_deviation': array([-75.], dtype=float32)},\n",
       " -3.0,\n",
       " 0,\n",
       " 0,\n",
       " {'skipped_requests': 3,\n",
       "  'off_schedule_trips': 0,\n",
       "  'time': 6837.0,\n",
       "  'veh_idx': 1,\n",
       "  'direction': 'out'})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rl_env.step(action=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'skipped_requests': -1.0, 'off_schedule_trips': -1.0}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from params import REWARD_WEIGHTS\n",
    "REWARD_WEIGHTS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-bus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
