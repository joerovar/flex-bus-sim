{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl_env import FlexSimEnv\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training_claude import train_vehicles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with shared policy...\n",
      "\n",
      "Training complete! Final average reward: -2.46\n",
      "\n",
      "Model saved as models/dqn_weight_4_5.pth\n",
      "Training with shared policy...\n",
      "\n",
      "Training complete! Final average reward: -2.57\n",
      "\n",
      "Model saved as models/dqn_weight_5_0.pth\n",
      "Training with shared policy...\n",
      "\n",
      "Training complete! Final average reward: -2.69\n",
      "\n",
      "Model saved as models/dqn_weight_5_5.pth\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 5e-3\n",
    "gamma = 0.98\n",
    "n_steps = 18_000\n",
    "\n",
    "reward_weights = [-4.5, -5.0, -5.5]\n",
    "model_paths = ['models/dqn_weight_4_5.pth', \n",
    "               'models/dqn_weight_5_0.pth',\n",
    "               'models/dqn_weight_5_5.pth']\n",
    "env = FlexSimEnv()\n",
    "for i in range(len(reward_weights)):\n",
    "    train_vehicles(reward_weight=reward_weights[i], \n",
    "                   learning_rate=learning_rate,\n",
    "                   gamma=gamma,\n",
    "                   n_steps=n_steps,\n",
    "                   save_path=model_paths[i],)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slope evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval import evaluate_slopes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating slope 0.0, reward_weight -2.0\n",
      "Evaluating slope 0.0, reward_weight -2.5\n",
      "Evaluating slope 0.0, reward_weight -3.0\n",
      "Evaluating slope 0.5, reward_weight -2.0\n",
      "Evaluating slope 0.5, reward_weight -2.5\n",
      "Evaluating slope 0.5, reward_weight -3.0\n",
      "Evaluating slope 1.0, reward_weight -2.0\n",
      "Evaluating slope 1.0, reward_weight -2.5\n",
      "Evaluating slope 1.0, reward_weight -3.0\n",
      "Evaluating slope 1.5, reward_weight -2.0\n",
      "Evaluating slope 1.5, reward_weight -2.5\n",
      "Evaluating slope 1.5, reward_weight -3.0\n",
      "Evaluating slope 2.0, reward_weight -2.0\n",
      "Evaluating slope 2.0, reward_weight -2.5\n",
      "Evaluating slope 2.0, reward_weight -3.0\n",
      "Evaluating slope 2.5, reward_weight -2.0\n",
      "Evaluating slope 2.5, reward_weight -2.5\n",
      "Evaluating slope 2.5, reward_weight -3.0\n",
      "Evaluating slope 3.0, reward_weight -2.0\n",
      "Evaluating slope 3.0, reward_weight -2.5\n",
      "Evaluating slope 3.0, reward_weight -3.0\n"
     ]
    }
   ],
   "source": [
    "reward_weights = [-2.0, -2.5, -3.0]\n",
    "slope_results = evaluate_slopes(reward_weights=reward_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope_df = pd.DataFrame(slope_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(slope_results).sort_values(by=['slope','mean_reward'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training_claude import grid_search_dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with shared policy...\n",
      "\n",
      "Training complete! Final average reward: -1.72\n",
      "\n",
      "Evaluation summary:\n",
      "Params: lr=0.006, n_steps=12000, gamma=0.985, reward_weight=-3.0\n",
      "Reward: -1.285 +/- 1.475\n",
      "------------------------\n",
      "Training with shared policy...\n",
      "\n",
      "Training complete! Final average reward: -1.66\n",
      "\n",
      "Evaluation summary:\n",
      "Params: lr=0.006, n_steps=12000, gamma=0.985, reward_weight=-3.2\n",
      "Reward: -2.568 +/- 2.533\n",
      "------------------------\n",
      "Training with shared policy...\n",
      "\n",
      "Training complete! Final average reward: -1.78\n",
      "\n",
      "Evaluation summary:\n",
      "Params: lr=0.006, n_steps=12000, gamma=0.985, reward_weight=-3.5\n",
      "Reward: -1.075 +/- 1.458\n",
      "------------------------\n",
      "Training with shared policy...\n",
      "\n",
      "Training complete! Final average reward: -1.73\n",
      "\n",
      "Evaluation summary:\n",
      "Params: lr=0.006, n_steps=12000, gamma=0.995, reward_weight=-3.0\n",
      "Reward: -2.682 +/- 2.408\n",
      "------------------------\n",
      "Training with shared policy...\n",
      "\n",
      "Training complete! Final average reward: -1.67\n",
      "\n",
      "Evaluation summary:\n",
      "Params: lr=0.006, n_steps=12000, gamma=0.995, reward_weight=-3.2\n",
      "Reward: -2.533 +/- 2.577\n",
      "------------------------\n",
      "Training with shared policy...\n",
      "\n",
      "Training complete! Final average reward: -1.74\n",
      "\n",
      "Evaluation summary:\n",
      "Params: lr=0.006, n_steps=12000, gamma=0.995, reward_weight=-3.5\n",
      "Reward: -2.649 +/- 2.737\n",
      "------------------------\n",
      "Training with shared policy...\n",
      "\n",
      "Training complete! Final average reward: -1.68\n",
      "\n",
      "Evaluation summary:\n",
      "Params: lr=0.006, n_steps=16000, gamma=0.985, reward_weight=-3.0\n",
      "Reward: -1.042 +/- 1.811\n",
      "------------------------\n",
      "Training with shared policy...\n",
      "\n",
      "Training complete! Final average reward: -1.59\n",
      "\n",
      "Evaluation summary:\n",
      "Params: lr=0.006, n_steps=16000, gamma=0.985, reward_weight=-3.2\n",
      "Reward: -1.112 +/- 1.512\n",
      "------------------------\n",
      "Training with shared policy...\n",
      "\n",
      "Training complete! Final average reward: -1.73\n",
      "\n",
      "Evaluation summary:\n",
      "Params: lr=0.006, n_steps=16000, gamma=0.985, reward_weight=-3.5\n",
      "Reward: -0.897 +/- 1.564\n",
      "------------------------\n",
      "Training with shared policy...\n",
      "\n",
      "Training complete! Final average reward: -1.65\n",
      "\n",
      "Evaluation summary:\n",
      "Params: lr=0.006, n_steps=16000, gamma=0.995, reward_weight=-3.0\n",
      "Reward: -2.4 +/- 2.392\n",
      "------------------------\n",
      "Training with shared policy...\n",
      "\n",
      "Training complete! Final average reward: -1.70\n",
      "\n",
      "Evaluation summary:\n",
      "Params: lr=0.006, n_steps=16000, gamma=0.995, reward_weight=-3.2\n",
      "Reward: -0.922 +/- 1.453\n",
      "------------------------\n",
      "Training with shared policy...\n",
      "\n",
      "Training complete! Final average reward: -1.82\n",
      "\n",
      "Evaluation summary:\n",
      "Params: lr=0.006, n_steps=16000, gamma=0.995, reward_weight=-3.5\n",
      "Reward: -1.333 +/- 2.194\n",
      "------------------------\n",
      "Training with shared policy...\n",
      "\n",
      "Training complete! Final average reward: -1.81\n",
      "\n",
      "Evaluation summary:\n",
      "Params: lr=0.006, n_steps=18000, gamma=0.985, reward_weight=-3.0\n",
      "Reward: -2.707 +/- 2.591\n",
      "------------------------\n",
      "Training with shared policy...\n",
      "\n",
      "Training complete! Final average reward: -1.62\n",
      "\n",
      "Evaluation summary:\n",
      "Params: lr=0.006, n_steps=18000, gamma=0.985, reward_weight=-3.2\n",
      "Reward: -2.648 +/- 2.539\n",
      "------------------------\n",
      "Training with shared policy...\n",
      "\n",
      "Training complete! Final average reward: -1.79\n",
      "\n",
      "Evaluation summary:\n",
      "Params: lr=0.006, n_steps=18000, gamma=0.985, reward_weight=-3.5\n",
      "Reward: -2.643 +/- 2.725\n",
      "------------------------\n",
      "Training with shared policy...\n",
      "\n",
      "Training complete! Final average reward: -1.60\n",
      "\n",
      "Evaluation summary:\n",
      "Params: lr=0.006, n_steps=18000, gamma=0.995, reward_weight=-3.0\n",
      "Reward: -0.881 +/- 1.437\n",
      "------------------------\n",
      "Training with shared policy...\n",
      "\n",
      "Training complete! Final average reward: -1.70\n",
      "\n",
      "Evaluation summary:\n",
      "Params: lr=0.006, n_steps=18000, gamma=0.995, reward_weight=-3.2\n",
      "Reward: -2.694 +/- 2.733\n",
      "------------------------\n",
      "Training with shared policy...\n",
      "\n",
      "Training complete! Final average reward: -1.83\n",
      "\n",
      "Evaluation summary:\n",
      "Params: lr=0.006, n_steps=18000, gamma=0.995, reward_weight=-3.5\n",
      "Reward: -2.617 +/- 2.763\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "grid_search_results = grid_search_dqn(\n",
    "    lr_values=[6e-3], # learning rate\n",
    "    n_steps_values=[12_000, 16_000, 18_000], # timesteps\n",
    "    gamma_values=[0.985, 0.995], # discount factor\n",
    "    reward_weights = [-3.0, -3.2, -3.5], # reward weights\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_df = pd.DataFrame(grid_search_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weight</th>\n",
       "      <th>lr</th>\n",
       "      <th>n_steps</th>\n",
       "      <th>gamma</th>\n",
       "      <th>mean_reward</th>\n",
       "      <th>std_reward</th>\n",
       "      <th>deviation_opportunities</th>\n",
       "      <th>deviations</th>\n",
       "      <th>avg_picked_requests</th>\n",
       "      <th>early_trips</th>\n",
       "      <th>late_trips</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-3.5</td>\n",
       "      <td>0.006</td>\n",
       "      <td>12000</td>\n",
       "      <td>0.995</td>\n",
       "      <td>-2.649</td>\n",
       "      <td>2.737</td>\n",
       "      <td>38.7</td>\n",
       "      <td>38.7</td>\n",
       "      <td>1.7</td>\n",
       "      <td>4.4</td>\n",
       "      <td>49.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-3.5</td>\n",
       "      <td>0.006</td>\n",
       "      <td>18000</td>\n",
       "      <td>0.985</td>\n",
       "      <td>-2.643</td>\n",
       "      <td>2.725</td>\n",
       "      <td>39.6</td>\n",
       "      <td>39.6</td>\n",
       "      <td>1.8</td>\n",
       "      <td>4.3</td>\n",
       "      <td>48.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-3.5</td>\n",
       "      <td>0.006</td>\n",
       "      <td>18000</td>\n",
       "      <td>0.995</td>\n",
       "      <td>-2.617</td>\n",
       "      <td>2.763</td>\n",
       "      <td>43.8</td>\n",
       "      <td>41.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>4.4</td>\n",
       "      <td>51.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-3.5</td>\n",
       "      <td>0.006</td>\n",
       "      <td>16000</td>\n",
       "      <td>0.995</td>\n",
       "      <td>-1.333</td>\n",
       "      <td>2.194</td>\n",
       "      <td>56.2</td>\n",
       "      <td>43.6</td>\n",
       "      <td>1.7</td>\n",
       "      <td>4.5</td>\n",
       "      <td>25.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-3.5</td>\n",
       "      <td>0.006</td>\n",
       "      <td>12000</td>\n",
       "      <td>0.985</td>\n",
       "      <td>-1.075</td>\n",
       "      <td>1.458</td>\n",
       "      <td>60.8</td>\n",
       "      <td>25.6</td>\n",
       "      <td>1.9</td>\n",
       "      <td>8.7</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-3.5</td>\n",
       "      <td>0.006</td>\n",
       "      <td>16000</td>\n",
       "      <td>0.985</td>\n",
       "      <td>-0.897</td>\n",
       "      <td>1.564</td>\n",
       "      <td>58.7</td>\n",
       "      <td>34.5</td>\n",
       "      <td>1.9</td>\n",
       "      <td>6.4</td>\n",
       "      <td>8.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-3.2</td>\n",
       "      <td>0.006</td>\n",
       "      <td>18000</td>\n",
       "      <td>0.995</td>\n",
       "      <td>-2.694</td>\n",
       "      <td>2.733</td>\n",
       "      <td>41.4</td>\n",
       "      <td>35.3</td>\n",
       "      <td>1.8</td>\n",
       "      <td>4.4</td>\n",
       "      <td>52.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-3.2</td>\n",
       "      <td>0.006</td>\n",
       "      <td>18000</td>\n",
       "      <td>0.985</td>\n",
       "      <td>-2.648</td>\n",
       "      <td>2.539</td>\n",
       "      <td>37.6</td>\n",
       "      <td>37.6</td>\n",
       "      <td>1.8</td>\n",
       "      <td>4.3</td>\n",
       "      <td>52.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-3.2</td>\n",
       "      <td>0.006</td>\n",
       "      <td>12000</td>\n",
       "      <td>0.985</td>\n",
       "      <td>-2.568</td>\n",
       "      <td>2.533</td>\n",
       "      <td>40.7</td>\n",
       "      <td>40.7</td>\n",
       "      <td>1.7</td>\n",
       "      <td>4.2</td>\n",
       "      <td>53.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-3.2</td>\n",
       "      <td>0.006</td>\n",
       "      <td>12000</td>\n",
       "      <td>0.995</td>\n",
       "      <td>-2.533</td>\n",
       "      <td>2.577</td>\n",
       "      <td>44.6</td>\n",
       "      <td>41.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>4.8</td>\n",
       "      <td>56.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-3.2</td>\n",
       "      <td>0.006</td>\n",
       "      <td>16000</td>\n",
       "      <td>0.985</td>\n",
       "      <td>-1.112</td>\n",
       "      <td>1.512</td>\n",
       "      <td>60.3</td>\n",
       "      <td>25.2</td>\n",
       "      <td>1.9</td>\n",
       "      <td>8.6</td>\n",
       "      <td>3.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-3.2</td>\n",
       "      <td>0.006</td>\n",
       "      <td>16000</td>\n",
       "      <td>0.995</td>\n",
       "      <td>-0.922</td>\n",
       "      <td>1.453</td>\n",
       "      <td>58.5</td>\n",
       "      <td>30.9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.4</td>\n",
       "      <td>7.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-3.0</td>\n",
       "      <td>0.006</td>\n",
       "      <td>18000</td>\n",
       "      <td>0.985</td>\n",
       "      <td>-2.707</td>\n",
       "      <td>2.591</td>\n",
       "      <td>46.2</td>\n",
       "      <td>38.5</td>\n",
       "      <td>1.7</td>\n",
       "      <td>3.4</td>\n",
       "      <td>62.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-3.0</td>\n",
       "      <td>0.006</td>\n",
       "      <td>12000</td>\n",
       "      <td>0.995</td>\n",
       "      <td>-2.682</td>\n",
       "      <td>2.408</td>\n",
       "      <td>40.2</td>\n",
       "      <td>39.1</td>\n",
       "      <td>1.7</td>\n",
       "      <td>3.7</td>\n",
       "      <td>56.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-3.0</td>\n",
       "      <td>0.006</td>\n",
       "      <td>16000</td>\n",
       "      <td>0.995</td>\n",
       "      <td>-2.400</td>\n",
       "      <td>2.392</td>\n",
       "      <td>41.1</td>\n",
       "      <td>39.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>4.9</td>\n",
       "      <td>52.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-3.0</td>\n",
       "      <td>0.006</td>\n",
       "      <td>12000</td>\n",
       "      <td>0.985</td>\n",
       "      <td>-1.285</td>\n",
       "      <td>1.475</td>\n",
       "      <td>62.7</td>\n",
       "      <td>19.5</td>\n",
       "      <td>1.8</td>\n",
       "      <td>10.9</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-3.0</td>\n",
       "      <td>0.006</td>\n",
       "      <td>16000</td>\n",
       "      <td>0.985</td>\n",
       "      <td>-1.042</td>\n",
       "      <td>1.811</td>\n",
       "      <td>54.9</td>\n",
       "      <td>42.6</td>\n",
       "      <td>1.7</td>\n",
       "      <td>5.7</td>\n",
       "      <td>20.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-3.0</td>\n",
       "      <td>0.006</td>\n",
       "      <td>18000</td>\n",
       "      <td>0.995</td>\n",
       "      <td>-0.881</td>\n",
       "      <td>1.437</td>\n",
       "      <td>60.0</td>\n",
       "      <td>32.6</td>\n",
       "      <td>1.9</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    weight     lr  n_steps  gamma  mean_reward  std_reward  \\\n",
       "5     -3.5  0.006    12000  0.995       -2.649       2.737   \n",
       "14    -3.5  0.006    18000  0.985       -2.643       2.725   \n",
       "17    -3.5  0.006    18000  0.995       -2.617       2.763   \n",
       "11    -3.5  0.006    16000  0.995       -1.333       2.194   \n",
       "2     -3.5  0.006    12000  0.985       -1.075       1.458   \n",
       "8     -3.5  0.006    16000  0.985       -0.897       1.564   \n",
       "16    -3.2  0.006    18000  0.995       -2.694       2.733   \n",
       "13    -3.2  0.006    18000  0.985       -2.648       2.539   \n",
       "1     -3.2  0.006    12000  0.985       -2.568       2.533   \n",
       "4     -3.2  0.006    12000  0.995       -2.533       2.577   \n",
       "7     -3.2  0.006    16000  0.985       -1.112       1.512   \n",
       "10    -3.2  0.006    16000  0.995       -0.922       1.453   \n",
       "12    -3.0  0.006    18000  0.985       -2.707       2.591   \n",
       "3     -3.0  0.006    12000  0.995       -2.682       2.408   \n",
       "9     -3.0  0.006    16000  0.995       -2.400       2.392   \n",
       "0     -3.0  0.006    12000  0.985       -1.285       1.475   \n",
       "6     -3.0  0.006    16000  0.985       -1.042       1.811   \n",
       "15    -3.0  0.006    18000  0.995       -0.881       1.437   \n",
       "\n",
       "    deviation_opportunities  deviations  avg_picked_requests  early_trips  \\\n",
       "5                      38.7        38.7                  1.7          4.4   \n",
       "14                     39.6        39.6                  1.8          4.3   \n",
       "17                     43.8        41.9                  1.7          4.4   \n",
       "11                     56.2        43.6                  1.7          4.5   \n",
       "2                      60.8        25.6                  1.9          8.7   \n",
       "8                      58.7        34.5                  1.9          6.4   \n",
       "16                     41.4        35.3                  1.8          4.4   \n",
       "13                     37.6        37.6                  1.8          4.3   \n",
       "1                      40.7        40.7                  1.7          4.2   \n",
       "4                      44.6        41.8                  1.7          4.8   \n",
       "7                      60.3        25.2                  1.9          8.6   \n",
       "10                     58.5        30.9                  2.0          7.4   \n",
       "12                     46.2        38.5                  1.7          3.4   \n",
       "3                      40.2        39.1                  1.7          3.7   \n",
       "9                      41.1        39.8                  1.7          4.9   \n",
       "0                      62.7        19.5                  1.8         10.9   \n",
       "6                      54.9        42.6                  1.7          5.7   \n",
       "15                     60.0        32.6                  1.9          7.0   \n",
       "\n",
       "    late_trips  \n",
       "5         49.1  \n",
       "14        48.2  \n",
       "17        51.4  \n",
       "11        25.3  \n",
       "2          3.0  \n",
       "8          8.1  \n",
       "16        52.9  \n",
       "13        52.4  \n",
       "1         53.4  \n",
       "4         56.1  \n",
       "7          3.9  \n",
       "10         7.4  \n",
       "12        62.2  \n",
       "3         56.8  \n",
       "9         52.6  \n",
       "0          2.4  \n",
       "6         20.2  \n",
       "15         6.7  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_df.sort_values(by=['weight', 'mean_reward'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_df.to_csv('outputs/grid_search_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test default training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with shared policy...\n",
      "Episode 0/500, Avg Score: -0.48, Epsilon: 1.0000\n",
      "Episode 20/500, Avg Score: -0.69, Epsilon: 0.8521\n",
      "Episode 40/500, Avg Score: -0.66, Epsilon: 0.7261\n",
      "Episode 60/500, Avg Score: -0.63, Epsilon: 0.6187\n",
      "Episode 80/500, Avg Score: -0.70, Epsilon: 0.5280\n",
      "Episode 100/500, Avg Score: -0.57, Epsilon: 0.4501\n",
      "Episode 120/500, Avg Score: -0.51, Epsilon: 0.3836\n",
      "Episode 140/500, Avg Score: -0.48, Epsilon: 0.3268\n",
      "Episode 160/500, Avg Score: -0.60, Epsilon: 0.2785\n",
      "Episode 180/500, Avg Score: -0.47, Epsilon: 0.2374\n",
      "Episode 200/500, Avg Score: -0.56, Epsilon: 0.2023\n",
      "Episode 220/500, Avg Score: -0.42, Epsilon: 0.1724\n",
      "Episode 240/500, Avg Score: -0.44, Epsilon: 0.1469\n",
      "Episode 260/500, Avg Score: -0.38, Epsilon: 0.1252\n",
      "Episode 280/500, Avg Score: -0.41, Epsilon: 0.1067\n",
      "Episode 300/500, Avg Score: -0.43, Epsilon: 0.0909\n",
      "Episode 320/500, Avg Score: -0.41, Epsilon: 0.0774\n",
      "Episode 340/500, Avg Score: -0.47, Epsilon: 0.0660\n",
      "Episode 360/500, Avg Score: -0.34, Epsilon: 0.0562\n",
      "Episode 380/500, Avg Score: -0.51, Epsilon: 0.0500\n",
      "Episode 400/500, Avg Score: -0.45, Epsilon: 0.0500\n",
      "Episode 420/500, Avg Score: -0.43, Epsilon: 0.0500\n",
      "Episode 440/500, Avg Score: -0.43, Epsilon: 0.0500\n",
      "Episode 460/500, Avg Score: -0.37, Epsilon: 0.0500\n",
      "Episode 480/500, Avg Score: -0.39, Epsilon: 0.0500\n",
      "\n",
      "Training complete! Final average reward: -0.40\n",
      "Model saved as 'shared_dqn_agent.pth'\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "from training_claude import train_vehicles\n",
    "\n",
    "agent, scores = train_vehicles(reward_weights={'off_schedule_trips': -4.0, 'skipped_requests': -1.0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: turn this into a function that by control stop, and number of requests creates scatterplots for headway and schedule deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For state {'control_stop_idx': array([0]), 'n_requests': array([2]), 'headway': array([700.5]), 'schedule_deviation': array([10])}, the model recommends action: 0\n"
     ]
    }
   ],
   "source": [
    "from training_claude import load_agent\n",
    "import numpy as np\n",
    "\n",
    "# Example of loading and using the model\n",
    "loaded_agent = load_agent('shared_dqn_agent.pth')\n",
    "\n",
    "# Example of getting an action from a state\n",
    "# The state should be a dictionary with the expected keys\n",
    "state = {\n",
    "    \"control_stop_idx\": np.array([0]),      # Make sure this is an array\n",
    "    \"n_requests\": np.array([2]),            # Make sure this is an array\n",
    "    \"headway\": np.array([700.5]),            # Make sure this is an array\n",
    "    \"schedule_deviation\": np.array([10])  # Make sure this is an array\n",
    "}\n",
    "vehicle_idx = 0  # Example vehicle index\n",
    "\n",
    "# Get action from loaded model\n",
    "action = loaded_agent.act(state, eval_mode=True)\n",
    "print(f\"For state {state}, the model recommends action: {action}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl_env import FlexSimEnv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(env, agent, num_episodes=10):\n",
    "    \"\"\"Evaluate the agent's performance in the environment.\"\"\"\n",
    "    results = {'pax': [], 'vehicles': [], 'state': [], 'idle': []}\n",
    "\n",
    "    np.random.seed(0)\n",
    "    for episode in range(num_episodes):\n",
    "        # Initialize state tracking for each vehicle\n",
    "        vehicle_observations = {}\n",
    "        vehicle_actions = {}  # Track previous actions\n",
    "        \n",
    "        # Start the episode\n",
    "        next_observation, info = env.reset()\n",
    "        vehicle_idx = info['veh_idx']\n",
    "\n",
    "        # update observation\n",
    "        observation = next_observation\n",
    "        vehicle_observations[vehicle_idx] = observation\n",
    "        \n",
    "        # select action\n",
    "        action = agent.act(observation, vehicle_idx)\n",
    "        vehicle_actions[vehicle_idx] = action\n",
    "\n",
    "        # take action in environment\n",
    "        next_observation, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        while not done:\n",
    "            # Get current vehicle index\n",
    "            vehicle_idx = info['veh_idx']\n",
    "            \n",
    "            # update observation\n",
    "            observation = next_observation\n",
    "            vehicle_observations[vehicle_idx] = observation\n",
    "\n",
    "            # Select action using shared policy\n",
    "            action = agent.act(observation, vehicle_idx)\n",
    "            vehicle_actions[vehicle_idx] = action\n",
    "            \n",
    "            # Take action in environment\n",
    "            next_observation, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "        # recordings\n",
    "        history = env.env.get_history()\n",
    "        for key in history:\n",
    "            history[key]['scenario'] = 'RL'\n",
    "            history[key]['episode'] = episode\n",
    "            results[key].append(history[key])\n",
    "    for df_key in results:\n",
    "        results[df_key] = pd.concat(results[df_key])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "from params import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = FlexSimEnv()\n",
    "results = evaluate_agent(env, loaded_agent, num_episodes=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_field_from_list_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = results['state'].copy()\n",
    "state = state[state['time'].between(RESULTS_START_TIME_MINUTES*60, RESULTS_END_TIME_MINUTES*60)]\n",
    "state['unweighted_rewards'] = state['unweighted_rewards'].astype(str)\n",
    "create_field_from_list_column(state, 0, 'skipped_requests', field_name='unweighted_rewards')\n",
    "create_field_from_list_column(state, 1, 'off_schedule_trips', field_name='unweighted_rewards')\n",
    "state['weighted_reward'] = state['skipped_requests'] * -1.0 + state['off_schedule_trips'] * -4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_raw = results['state'].copy()\n",
    "# state_raw.groupby(['episode']).size().head() # confirm it's 65 control steps per episode\n",
    "num_time_steps = 10_000\n",
    "time_steps_per_episode = 65\n",
    "num_episodes = int(num_time_steps/time_steps_per_episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## toy with environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_env  = FlexSimEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<rl_env.FlexSimEnv at 0x171bcb690>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_env.route."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'control_stop_idx': array([0], dtype=int32),\n",
       "  'n_requests': array([0], dtype=int32),\n",
       "  'headway': array([594.], dtype=float32),\n",
       "  'schedule_deviation': array([-55.], dtype=float32)},\n",
       " {'skipped_requests': 0,\n",
       "  'off_schedule_trips': 0,\n",
       "  'time': 705.0,\n",
       "  'veh_idx': 1,\n",
       "  'direction': 'out'})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rl_env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'control_stop_idx': array([1], dtype=int32),\n",
       "  'n_requests': array([1], dtype=int32),\n",
       "  'headway': array([495.], dtype=float32),\n",
       "  'schedule_deviation': array([-75.], dtype=float32)},\n",
       " -3.0,\n",
       " 0,\n",
       " 0,\n",
       " {'skipped_requests': 3,\n",
       "  'off_schedule_trips': 0,\n",
       "  'time': 6837.0,\n",
       "  'veh_idx': 1,\n",
       "  'direction': 'out'})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rl_env.step(action=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'skipped_requests': -1.0, 'off_schedule_trips': -1.0}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from params import REWARD_WEIGHTS\n",
    "REWARD_WEIGHTS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-bus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
